{
    "collab_server" : "",
    "contents" : "# Scripts for Kaggle competition Digit Recognizer\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(randomForest)\nlibrary(mxnet)\n\nsetwd(\"~/kaggle/digit_recognizer/\")\n\nload_data <- function(train_path = \"data/train.csv\",\n                      test_path = \"data/test.csv\",\n                      val_path = NA,\n                      val_prop = 0.3,\n                      seed = 0) {\n  #' load data from raw file\n  #' \n  #' @param train_path String, the path of training data\n  #' @param test_path String, the path of test data\n  #' @param val_path String, optional, the path of validation data, if not \n  #' supplied, the training data will be divided into training and validation\n  #' data\n  #' @param val_prop Float, from 0 to 1,if validation data is not supplied,\n  #'  the val_prop proportion of training data will serve as validation data\n  #'  @param seed Int, arguments for set.seed()\n  #'  @return list with three elements (training, test and validation data)\n  train <- fread(train_path)\n  test <- fread(test_path)\n  if (is.na(val_path)) {\n    if (val_prop < 0 | val_prop >= 1) stop(\"Invalid validation \n                                           proportion (val_prop)\")\n    if (val_prop == 0) {\n      warning(\"No validation data\")\n      return(list(train, test, NA))\n    }\n    set.seed(seed)\n    train_number <- nrow(train)\n    train_row <- sample(train_number, floor(train_number * (1 - val_prop)),\n                        replace = F)\n    val_row <- setdiff(1:train_number, train_row)\n    val <- train[val_row, ]\n    train <- train[train_row, ]\n  } else {\n    val <- fread(val_path)\n  }\n  return(list(train, test, val))\n  }\n\n# load training, validation and test data---------------------------------------\nall_data <- load_data()\ntrain <- as.data.table(data.frame(all_data[1]))\ntest <- as.data.table(data.frame(all_data[2]))\nvalid <- as.data.table(data.frame(all_data[3]))\n\n# benchmark validation error by random forest-----------------------------------\ntrain_x <- as.matrix(train[, 2:ncol(train), with = F])\ntrain_y <- as.factor(train[[1]])\nvalid_x <- as.matrix(valid[, 2:ncol(valid), with = F])\nvalid_y <- as.factor(valid[[1]])\ntrain_all <- rbind(train, valid)\ntrain_all_x <- as.matrix(train_all[, 2:ncol(train_all), with = F])\ntrain_all_y <- as.factor(train_all[[1]])\ntest_x <- as.matrix(test)\n\nnum_trees <- 25\nrf <- randomForest(x = train_x, y = train_y, xtest = valid_x, ytest = valid_y,\n                   ntree = num_trees)\nrf_accuracy <- 1 - rf$test$err.rate[num_trees, 1]\n\n# mlp---------------------------------------------------------------------------\nmlp <- mx.mlp(train_x, as.numeric(as.character(train_y)), \n              hidden_node = c(500, 300), out_node = 11,\n              learning.rate = 0.01, dropout = 0.7)\nvalid_y_prediction <- as.factor(apply(predict(mlp, valid_x), 2, which.max) - 1)\nmlp_accuracy <- sum(valid_y_prediction == valid_y) / length(valid_y)\n\n# convolutional network---------------------------------------------------------\ntrain_array <- t(train_x)\ndim(train_array) <- c(28, 28, 1, nrow(train_x))\nvalid_array <- t(valid_x)\ndim(valid_array) <- c(28, 28, 1, nrow(valid_x))\ntrain_all_array <- t(train_all_x)\ndim(train_all_array) <- c(28, 28, 1, nrow(train_all_x))\ntest_array <- t(test_x)\ndim(test_array) <- c(28, 28, 1, nrow(test_x))\n\ndata <- mx.symbol.Variable(\"data\")\n# first convolutional layer\nconv1 <- mx.symbol.Convolution(data = data, kernel = c(5, 5), num_filter = 10)\nconv1_activate <- mx.symbol.Activation(data = conv1, act_type = \"relu\")\npool1 <- mx.symbol.Pooling(data = conv1_activate, pool_type = \"max\",\n                           kernel = c(2, 2), stride = c(2, 2))\n# second convolutional layer\nconv2 <- mx.symbol.Convolution(data = pool1, kernel = c(5, 5), num_filter = 20)\nconv2_activate <- mx.symbol.Activation(data = conv2, act_type = \"relu\")\npool2 <- mx.symbol.Pooling(data = conv2_activate, pool_type = \"max\",\n                           kernel = c(2, 2), stride = c(2, 2))\n# first full connected layer\nflatten <- mx.symbol.Flatten(pool2)\nfc1 <- mx.symbol.FullyConnected(data = flatten, num_hidden = 100)\nfc1_activate <- mx.symbol.Activation(data = fc1, act_type = \"relu\")\n# output layer\nfc2 <- mx.symbol.FullyConnected(data = fc1_activate, num_hidden = 10)\ncost <- mx.symbol.SoftmaxOutput(data = fc2)\n\n\n# cnn <- mx.model.FeedForward.create(\n#   cost, X = train_array, y = train_y,\n#   num.round = 10, array.batch.size = 100,\n#   learning.rate = 0.05, momentum = 0.9, wd = 0.00001,\n#   eval.metric=mx.metric.accuracy,\n#   epoch.end.callback = mx.callback.log.train.metric(100))\ncnn <- mx.model.FeedForward.create(\n  cost, \n  X = train_array, y = as.numeric(as.character(train_y)),\n  num.round = 10, array.batch.size = 100,\n  learning.rate = 0.05,\n  eval.metric = mx.metric.accuracy)\ncnn_pred <- predict(cnn, valid.array)\npred_label <- max.col(t(cnn_pred)) - 1\nvalid_accuracy <- sum(as.numeric(as.character(valid_y)) == pred_label) / \n  length(pred_label)\n# train model with all data\ncnn <- mx.model.FeedForward.create(\n  cost, \n  X = train_all_array, y = as.numeric(as.character(train_all_y)),\n  num.round = 10, array.batch.size = 100,\n  learning.rate = 0.05,\n  eval.metric = mx.metric.accuracy)\ntest_pred <- predict(cnn, test_array)\nsubmit <- data.table(\n  ImageID = 1:length(test_pred),\n  Label = test_pred\n)\nwrite.csv(submit, file = \"data/submission_1.csv\", row.names = F)\n",
    "created" : 1467541770595.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "12|33|44|2|\n",
    "hash" : "1316524061",
    "id" : "C2C8FA1F",
    "lastKnownWriteTime" : 1467772510,
    "last_content_update" : 1467772511413,
    "path" : "~/kaggle/digit_recognizer/R/digit_recognizer.R",
    "project_path" : "R/digit_recognizer.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}