{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DateSets():\n",
    "    \"\"\"The datasets used to train, validate and test the model\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_features=None, n_labels=None):\n",
    "        \"\"\"Load training data, validation data and test data\n",
    "        \n",
    "        Args:\n",
    "            n_features: the number of features\n",
    "            n_labels: the number of labels\n",
    "        \"\"\"\n",
    "        self.training_x = None\n",
    "        self.training_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.validation_x = None\n",
    "        self.validation_y = None\n",
    "        self.all_x = None\n",
    "        self.all_y = None\n",
    "        self.n_features = n_features\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        \n",
    "    def load_training(self, training_x, training_y):\n",
    "        \"\"\"Load training data\n",
    "        \n",
    "        Args:\n",
    "            training_x: features of training data\n",
    "            training_y: labels of training data\n",
    "        \"\"\"\n",
    "        assert training_x.shape[0] == len(training_y)\n",
    "        self.training_x = training_x\n",
    "        self.training_y = training_y\n",
    "        if self.n_features is None:\n",
    "            self.n_features = training_x.shape[1]\n",
    "        if self.n_labels is None:\n",
    "            self.n_labels = len(set(training_y))\n",
    "        self.training_sample_size = training_x.shape[0]\n",
    "        \n",
    "        \n",
    "    def load_test(self, test_x, test_y):\n",
    "        \"\"\"Load training data\n",
    "        \n",
    "        Args:\n",
    "            test_x: feature of test data\n",
    "            test_y: labels of test data\n",
    "        \"\"\"\n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        \n",
    "        \n",
    "    def load_validation(self, validation_x, validation_y):\n",
    "        \"\"\"Load training data\n",
    "        \n",
    "        Args:\n",
    "            validation_x: features of validation data\n",
    "            validation_y: labels of validation data\n",
    "        \"\"\"\n",
    "        self.validation_x = validation_x\n",
    "        self.validation_y = validation_y\n",
    "        \n",
    "        \n",
    "    def load_all(self, all_x, all_y):\n",
    "        \"\"\"Loading all the data to train the final model\n",
    "        \n",
    "        Args:\n",
    "            all_x: features of all data\n",
    "            all_y: labels of all data\n",
    "        \"\"\"\n",
    "        self.all_x = all_x\n",
    "        self.all_y = all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedHiddenLayer():\n",
    "    \"\"\"A fully connected hidden layer\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('/Users/yinan/kaggle/digit_recognizer/data/train.csv')\n",
    "raw_data = raw_data.reindex(np.random.permutation(raw_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 785)\n",
      "(4200, 785)\n",
      "(4200, 785)\n"
     ]
    }
   ],
   "source": [
    "sample_num = raw_data.shape[0]\n",
    "training_data = raw_data.iloc[:int(sample_num * 0.8), :]\n",
    "validation_data = raw_data.iloc[int(sample_num * 0.8):int(sample_num * 0.9), :]\n",
    "test_data = raw_data.iloc[int(sample_num * 0.9):, :]\n",
    "print(training_data.shape)\n",
    "print(validation_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = DateSets(n_features=784, n_labels=10)\n",
    "mnist.load_training(training_data.iloc[:, 1:].values / 255, training_data.iloc[:, 0].values)\n",
    "mnist.load_validation(validation_data.iloc[:, 1:].values / 255, validation_data.iloc[:, 0].values)\n",
    "mnist.load_test(test_data.iloc[:, 1:].values / 255, test_data.iloc[:, 0].values)\n",
    "mnist.load_all(raw_data.iloc[:, 1:].values / 255, raw_data.iloc[:, 0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP  (97.05% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "data_x = tf.placeholder(tf.float32, shape=(None, mnist.n_features))\n",
    "data_y = tf.placeholder(tf.int32, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden1_nodes = 128\n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([mnist.n_features, n_hidden1_nodes], stddev=1.0 / np.sqrt(mnist.n_features)),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(\n",
    "        tf.zeros([n_hidden1_nodes]),\n",
    "        name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(data_x, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden2_nodes = 32\n",
    "with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden1_nodes, n_hidden2_nodes], stddev=1.0 / np.sqrt(n_hidden1_nodes)),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(\n",
    "        tf.zeros([n_hidden2_nodes]),\n",
    "        name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden2_nodes, mnist.n_labels], stddev=1.0 / np.sqrt(n_hidden2_nodes)),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(\n",
    "        tf.zeros([mnist.n_labels]),\n",
    "        name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(logits)\n",
    "accuracy = tf.nn.in_top_k(logits, data_y, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, data_y, name='cross_entropy')\n",
    "loss = tf.reduce_mean(cross_entropy, name='cross_entropy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "checkpoint_file = './checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.scalar_summary(loss.op.name, loss)\n",
    "summary_op = tf.merge_all_summaries()\n",
    "summary_writer = tf.train.SummaryWriter('./', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step 1000, the loss is 0.1622794270515442.\n",
      "The accuracy on validation set is 0.9621428571428572.\n",
      "In step 2000, the loss is 0.009996295906603336.\n",
      "The accuracy on validation set is 0.9633333333333334.\n",
      "In step 3000, the loss is 0.10839683562517166.\n",
      "The accuracy on validation set is 0.9638095238095238.\n",
      "In step 4000, the loss is 0.057765811681747437.\n",
      "The accuracy on validation set is 0.9664285714285714.\n",
      "In step 5000, the loss is 0.06623770296573639.\n",
      "The accuracy on validation set is 0.9673809523809523.\n",
      "In step 6000, the loss is 0.021750975400209427.\n",
      "The accuracy on validation set is 0.9688095238095238.\n",
      "In step 7000, the loss is 0.021516375243663788.\n",
      "The accuracy on validation set is 0.9697619047619047.\n",
      "In step 8000, the loss is 0.006869563367217779.\n",
      "The accuracy on validation set is 0.9688095238095238.\n",
      "In step 9000, the loss is 0.004652712494134903.\n",
      "The accuracy on validation set is 0.97.\n",
      "In step 10000, the loss is 0.014274745248258114.\n",
      "The accuracy on validation set is 0.969047619047619.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoint-10752'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = int((mnist.training_x.shape[0] / batch_size))\n",
    "max_epoches = 32\n",
    "steps = 0\n",
    "for epoch in range(max_epoches):\n",
    "    permutation_index = np.random.permutation(range(mnist.training_sample_size))\n",
    "    training_x = mnist.training_x[permutation_index]\n",
    "    training_y = mnist.training_y[permutation_index]\n",
    "    for i in range(n_batches):\n",
    "        steps += 1\n",
    "        feed_dict = {data_x: training_x[(i * batch_size): ((i + 1) * batch_size)],\n",
    "                     data_y: training_y[(i * batch_size): ((i + 1) * batch_size)]}\n",
    "        _, loss_value = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, steps)\n",
    "            summary_writer.flush()\n",
    "        \n",
    "        if steps % 1000 == 0:\n",
    "            print('In step {}, the loss is {}.'.format(steps, loss_value))\n",
    "            valid_feed_dict = {data_x: mnist.validation_x,\n",
    "                               data_y: mnist.validation_y}\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict=valid_feed_dict)\n",
    "            accuracy_rate = sum(valid_accuracy) / len(valid_accuracy)\n",
    "            print('The accuracy on validation set is {}.'.format(accuracy_rate))\n",
    "            saver.save(sess, checkpoint_file, global_step=steps)\n",
    "saver.save(sess, checkpoint_file, global_step=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test set is 0.9704761904761905.\n"
     ]
    }
   ],
   "source": [
    "test_feed_dict = {data_x: mnist.test_x, data_y: mnist.test_y}\n",
    "test_accuracy = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "accuracy_rate = sum(test_accuracy) / len(test_accuracy)\n",
    "print('The accuracy on test set is {}.'.format(accuracy_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (98.79% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "data_x = tf.placeholder(tf.float32, shape=(None, np.sqrt(mnist.n_features), np.sqrt(mnist.n_features), 1))\n",
    "data_y = tf.placeholder(tf.int32, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros([32]))\n",
    "    conv = tf.nn.conv2d(data_x, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv = tf.nn.relu(conv + biases)\n",
    "pooling1 = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros([64]))\n",
    "    conv = tf.nn.conv2d(pooling1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    h_conv = tf.nn.relu(conv + biases)\n",
    "pooling2 = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pooling2_shape = list(pooling2.get_shape())\n",
    "fc1_input_shape = int(pooling2_shape[1] * pooling2_shape[2] * pooling2_shape[3])\n",
    "pooling2_flat = tf.reshape(pooling2, [-1, fc1_input_shape])\n",
    "with tf.variable_scope('fc1') as scope:\n",
    "    weights = tf.Variable(tf.truncated_normal([fc1_input_shape, 1024], stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros([1024]))\n",
    "    fc1 = tf.nn.relu(tf.matmul(pooling2_flat, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_out') as scope:\n",
    "    weights = tf.Variable(tf.truncated_normal([1024, mnist.n_labels], stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros([mnist.n_labels]))\n",
    "    logits = tf.matmul(fc1_drop, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, data_y), name='loss')\n",
    "pred = tf.nn.softmax(logits)\n",
    "accuracy = tf.nn.in_top_k(logits, data_y, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_x_flat = np.reshape(mnist.validation_x, [mnist.validation_x.shape[0], 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step 1000, the loss is 0.1103094294667244.\n",
      "The accuracy on validation set is 0.9730952380952381.\n",
      "In step 2000, the loss is 0.05188584327697754.\n",
      "The accuracy on validation set is 0.9835714285714285.\n",
      "In step 3000, the loss is 0.02617611177265644.\n",
      "The accuracy on validation set is 0.985.\n",
      "In step 4000, the loss is 0.007313924841582775.\n",
      "The accuracy on validation set is 0.9859523809523809.\n",
      "In step 5000, the loss is 0.011613022536039352.\n",
      "The accuracy on validation set is 0.9873809523809524.\n",
      "In step 6000, the loss is 0.03831010311841965.\n",
      "The accuracy on validation set is 0.9876190476190476.\n",
      "In step 7000, the loss is 0.0010470832930877805.\n",
      "The accuracy on validation set is 0.9883333333333333.\n",
      "In step 8000, the loss is 0.004456762224435806.\n",
      "The accuracy on validation set is 0.9885714285714285.\n",
      "In step 9000, the loss is 0.030779356136918068.\n",
      "The accuracy on validation set is 0.9888095238095238.\n",
      "In step 10000, the loss is 0.00938940979540348.\n",
      "The accuracy on validation set is 0.9876190476190476.\n"
     ]
    }
   ],
   "source": [
    "n_batches = int((mnist.training_x.shape[0] / batch_size))\n",
    "max_epoches = 32\n",
    "steps = 0\n",
    "for epoch in range(max_epoches):\n",
    "    permutation_index = np.random.permutation(range(mnist.training_sample_size))\n",
    "    training_x = mnist.training_x[permutation_index]\n",
    "    training_y = mnist.training_y[permutation_index]\n",
    "    training_x = np.reshape(training_x, [training_x.shape[0], 28, 28, 1])\n",
    "    for i in range(n_batches):\n",
    "        steps += 1\n",
    "        feed_dict = {data_x: training_x[(i * batch_size): ((i + 1) * batch_size)],\n",
    "                     data_y: training_y[(i * batch_size): ((i + 1) * batch_size)],\n",
    "                     keep_prob: 0.5\n",
    "                    }\n",
    "        _, loss_value = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if steps % 1000 == 0:\n",
    "            print('In step {}, the loss is {}.'.format(steps, loss_value))\n",
    "            valid_feed_dict = {data_x: validation_x_flat,\n",
    "                               data_y: mnist.validation_y,\n",
    "                               keep_prob: 1\n",
    "                              }\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict=valid_feed_dict)\n",
    "            accuracy_rate = sum(valid_accuracy) / len(valid_accuracy)\n",
    "            print('The accuracy on validation set is {}.'.format(accuracy_rate))\n",
    "print('Finish training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test set is 0.9878571428571429.\n"
     ]
    }
   ],
   "source": [
    "test_feed_dict = {data_x: np.reshape(mnist.test_x, [mnist.test_x.shape[0], 28, 28, 1]), \n",
    "                  data_y: mnist.test_y,\n",
    "                  keep_prob: 1\n",
    "                 }\n",
    "test_accuracy = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "accuracy_rate = sum(test_accuracy) / len(test_accuracy)\n",
    "print('The accuracy on test set is {}.'.format(accuracy_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict kaggle test data (99.13% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step 1000, the loss is 0.08695012331008911.\n",
      "The accuracy on this batch is 0.96.\n",
      "In step 2000, the loss is 0.03808596730232239.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 3000, the loss is 0.02897307090461254.\n",
      "The accuracy on this batch is 0.99.\n",
      "In step 4000, the loss is 0.02167578972876072.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 5000, the loss is 0.003554876195266843.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 6000, the loss is 0.007254057098180056.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 7000, the loss is 0.015092527493834496.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 8000, the loss is 0.022795602679252625.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 9000, the loss is 0.0023500279057770967.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 10000, the loss is 0.0030890898779034615.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 11000, the loss is 0.0005180800217203796.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 12000, the loss is 0.006005151197314262.\n",
      "The accuracy on this batch is 1.0.\n",
      "In step 13000, the loss is 0.008945265784859657.\n",
      "The accuracy on this batch is 1.0.\n",
      "Finish training.\n"
     ]
    }
   ],
   "source": [
    "n_batches = int((mnist.all_x.shape[0] / batch_size))\n",
    "max_epoches = 32\n",
    "steps = 0\n",
    "for epoch in range(max_epoches):\n",
    "    permutation_index = np.random.permutation(range(mnist.all_x.shape[0]))\n",
    "    training_x = mnist.all_x[permutation_index]\n",
    "    training_y = mnist.all_y[permutation_index]\n",
    "    training_x = np.reshape(training_x, [training_x.shape[0], 28, 28, 1])\n",
    "    for i in range(n_batches):\n",
    "        steps += 1\n",
    "        batch_x = training_x[(i * batch_size): ((i + 1) * batch_size)]\n",
    "        batch_y = training_y[(i * batch_size): ((i + 1) * batch_size)]\n",
    "        feed_dict = {data_x: batch_x,\n",
    "                     data_y: batch_y,\n",
    "                     keep_prob: 0.5\n",
    "                    }\n",
    "        _, loss_value = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if steps % 1000 == 0:\n",
    "            print('In step {}, the loss is {}.'.format(steps, loss_value))\n",
    "            valid_feed_dict = {data_x: batch_x,\n",
    "                               data_y: batch_y,\n",
    "                               keep_prob: 1\n",
    "                              }\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict=valid_feed_dict)\n",
    "            accuracy_rate = sum(valid_accuracy) / len(valid_accuracy)\n",
    "            print('The accuracy on this batch is {}.'.format(accuracy_rate))\n",
    "print('Finish training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_test = pd.read_csv('/Users/yinan/kaggle/digit_recognizer/data/test.csv')\n",
    "kaggle_test = kaggle_test.values / 255\n",
    "kaggle_test = np.reshape(kaggle_test, [-1, 28, 28, 1])\n",
    "\n",
    "test_pred = sess.run(tf.arg_max(pred, dimension=1), feed_dict={data_x: kaggle_test, keep_prob: 1})\n",
    "\n",
    "submission = pd.DataFrame([list(range(1, kaggle_test.shape[0] + 1)), list(test_pred)]).transpose()\n",
    "submission.columns = ['ImageID', 'Label']\n",
    "\n",
    "submission.to_csv('/Users/yinan/kaggle/digit_recognizer/data/submission_20160824.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
